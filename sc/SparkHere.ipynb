{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment required one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "APP_NAME = 'Twitter Stat'\n",
    "# Standalone version:\n",
    "SPARK_ADDRESS = \"local[*]\"\n",
    "RAW_DATA_PATH = '../data/raw_data.csv'\n",
    "TMP_DIR_PATH = '../data/'\n",
    "OUTPUT_PATH = '../data/USER_SENT_TOPIC.parquet'\n",
    "\n",
    "# Multi-node version:\n",
    "# SPARK_ADDRESS = \"spark://192.168.13.133:7077\"\n",
    "# HDFS_ADDRESS = \"hdfs://192.168.13.133:8020/\"\n",
    "# os.environ[\"HADOOP_CONF_DIR\"] = \"/usr/local/hadoop/conf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0: From SQLite to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timur, here's tha place for your code: getting data from db and saving to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using pandas for simplification the rename process and dealing with types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upd_columns = {'id_str': 'id', 'screename': 'username', 'created_at': 'date_time', \n",
    "               'pic': 'picture'}\n",
    "raw_data = pd.read_csv(RAW_DATA_PATH, encoding='utf-8', parse_dates=['created_at', 'creation']).drop(\"Unnamed: 0\", axis=1)\n",
    "raw_data.rename(columns=upd_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data.to_parquet(TMP_DIR_PATH+'tmp_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: pySpark Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Require for using pySpark in jupiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pySpark imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "# Data Aggregation:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "# Topic extraction\n",
    "from pyspark.ml.linalg import Vectors, SparseVector, DenseVector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.column import _to_java_column, _to_seq, Column\n",
    "# LDA\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, IDF, HashingTF\n",
    "from pyspark.ml.clustering import LDA, DistributedLDAModel\n",
    "# Clustering\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .enableHiveSupport()\\\n",
    "    .master(SPARK_ADDRESS)\\\n",
    "    .appName(APP_NAME)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Work with Data in pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark\\\n",
    "    .read\\\n",
    "    .parquet(TMP_DIR_PATH+'tmp_data.parquet')\\\n",
    "    .drop('__index_level_0__')\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data modification, features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate aggreagated stats per user based on the sentiment column from each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wind = Window\\\n",
    "    .partitionBy(col(\"username\"))\n",
    "splitting = udf(lambda t: [(int(x.split(' : ')[0]) if x.split(' : ')[0]!='None' \\\n",
    "                            else int(-1)) for x in t.split(', ') if t!=-2], ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.withColumn(\"id\", col(\"id\").cast(LongType()))\\\n",
    "            .withColumn(\"Positive\", \n",
    "                       struct(\n",
    "                        col(\"positive_score\").alias('score'),\n",
    "                        variance(col(\"positive_score\")).over(wind).alias(\"var\"),\n",
    "                        mean(col(\"positive_score\")).over(wind).alias(\"mean\"),\n",
    "                        max(col(\"positive_score\")).over(wind).alias(\"max\"),\n",
    "                        min(col(\"positive_score\")).over(wind).alias(\"min\"),\n",
    "                        count(col(\"positive_score\")).over(wind).alias(\"cnt\"))).drop(\"positive_score\")\\\n",
    "            .withColumn(\"Neutral\", \n",
    "                       struct(\n",
    "                        col(\"neutral_score\").alias('score'),\n",
    "                        variance(col(\"neutral_score\")).over(wind).alias(\"var\"),\n",
    "                        mean(col(\"neutral_score\")).over(wind).alias(\"mean\"),\n",
    "                        max(col(\"neutral_score\")).over(wind).alias(\"max\"),\n",
    "                        min(col(\"neutral_score\")).over(wind).alias(\"min\"),\n",
    "                        count(col(\"neutral_score\")).over(wind).alias(\"cnt\"))).drop(\"neutral_score\")\\\n",
    "            .withColumn(\"Negative\", \n",
    "                       struct(\n",
    "                        col(\"negative_score\").alias('score'),\n",
    "                        variance(col(\"negative_score\")).over(wind).alias(\"var\"),\n",
    "                        mean(col(\"negative_score\")).over(wind).alias(\"mean\"),\n",
    "                        max(col(\"negative_score\")).over(wind).alias(\"max\"),\n",
    "                        min(col(\"negative_score\")).over(wind).alias(\"min\"),\n",
    "                        count(col(\"negative_score\")).over(wind).alias(\"cnt\"))).drop(\"negative_score\")\\\n",
    "            .na.fill({'retweets_id': -2, 'likes_id': -2})\\\n",
    "                    .withColumn('retweeted_by', splitting('retweets_id'))\\\n",
    "                    .withColumn('liked_by', splitting('likes_id')).drop('retweets_id', 'likes_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "try: \n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    import nltk\n",
    "    print(\"Download NLTK data...\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "m = pymystem3.Mystem()\n",
    "\n",
    "stop = ['.', '\"', ',', '!', \n",
    "        '?', ';', ':', '-', '&', 'â€”', \"'\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "service_tags = [\"<tag>\", \"<reply>\", \"<num>\", \"<url>\", \"<smile>\", \"<sad>\"]\n",
    "\n",
    "def merge_text(raw):\n",
    "    \"\"\"Merging text from all tweets\"\"\"\n",
    "    text = ' '.join(raw)\n",
    "    return text    \n",
    "merge_text_udf = udf(merge_text, StringType())\n",
    "\n",
    "def separate(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "    text = [word for word in text.split(' ') if word != '']\n",
    "    return text\n",
    "separate_udf = udf(separate, ArrayType(StringType()))\n",
    "\n",
    "def lemmatize(word):\n",
    "    if word in service_tags:\n",
    "        return word\n",
    "    else:\n",
    "        return m.lemmatize(word)[0]\n",
    "\n",
    "def clean_word(word):\n",
    "    if word.isalpha():\n",
    "        return word\n",
    "    elif word in service_tags:\n",
    "        return word\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def process_punct(text):\n",
    "    for char in stop:\n",
    "        text = text.replace(char, ' ')\n",
    "    return text\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower().replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"\\[\\S+\\]\", \" <reply>\", text)\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\", text)\n",
    "    text = re.sub(r\"#(\\w+)\", \" <tag>\", text)\n",
    "    text = re.sub(\"\\d+\", \" <num>\", text)\n",
    "    text = re.sub(r\"\\(+\", \" <sad>\", text)\n",
    "    text = re.sub(r\"\\)+\", \" <smile>\", text)\n",
    "    text = process_punct(text)   \n",
    "    tokens = text.split(\" \")\n",
    "    tokens = [token for token in tokens if token != '']\n",
    "    tokens = [lemmatize(x) for x in tokens]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [clean_word(token) for token in tokens]\n",
    "    return \" \".join(tokens).strip()\n",
    "\n",
    "process_text_udf = udf(lambda x: process_text(x), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "max_iterations = 30\n",
    "vocab_size = 5000\n",
    "min_tf = 5\n",
    "    \n",
    "lda_model_path = TMP_DIR_PATH + \"topics/lda.model\"\n",
    "lda_topics_path = TMP_DIR_PATH + \"topics/lda_topics\"\n",
    "lda_transformed_docs_path = TMP_DIR_PATH + \"topics/lda_transformed_docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lda(df, num_topics, max_iterations):\n",
    "    lda = LDA(k=num_topics, seed=123, optimizer=\"em\",\n",
    "              maxIter=max_iterations, featuresCol=\"features\")\n",
    "    lda_model = lda.fit(df)\n",
    "    return lda_model\n",
    "\n",
    "def write_topics_df(lda_model, cv_model, num_topics, lda_topics_path):\n",
    "    print(\"Building vocabulary\")\n",
    "    vocabulary = cv_model.vocabulary\n",
    "    idx_to_text = lambda idx: [vocabulary[int(x)] for x in idx]\n",
    "    idx_to_text_udf = udf(idx_to_text, ArrayType(StringType()))\n",
    "    \n",
    "    terms_count = len(vocabulary)\n",
    "    \n",
    "    topics = lda_model\\\n",
    "        .describeTopics(terms_count)\\\n",
    "        .withColumn(\"terms\", idx_to_text_udf(\"termIndices\"))\\\n",
    "        .select(\"topic\", \"terms\", \"termIndices\", \"termWeights\")\n",
    "    topics.write\\\n",
    "        .parquet(lda_topics_path, mode='overwrite')\n",
    "\n",
    "def perform_topic_modelling(posts, num_topics, max_iterations, \n",
    "                            lda_model_path, lda_topics_path, lda_transformed_docs_path,\n",
    "                            text_col=\"text\"):\n",
    "    posts_aggregated = posts\n",
    "    \n",
    "    print(\"Training LDA model\")\n",
    "    lda_model = train_lda(posts_aggregated, num_topics, max_iterations)\n",
    "    lda_model.write().overwrite()\\\n",
    "        .save(lda_model_path)\n",
    "    \n",
    "    print(\"Writing topics\")\n",
    "    write_topics_df(lda_model, cv_model, num_topics, lda_topics_path)\n",
    "    \n",
    "    print(\"Inferring topics mixture for individual posts\")\n",
    "    posts_transformed = lda_model.transform(posts_aggregated)\n",
    "    \n",
    "    main_topic_idx = lambda l: int(np.argmax([float(x) for x in l]))\n",
    "    main_topic_idx_udf = udf(main_topic_idx, IntegerType())\n",
    "    \n",
    "    posts_transformed = posts_transformed\\\n",
    "        .withColumn(\"main_topic\", main_topic_idx_udf(\"topicDistribution\"))\\\n",
    "        \n",
    "    posts_transformed\\\n",
    "        .write\\\n",
    "        .parquet(lda_transformed_docs_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temp RDD for LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data4topic = data.select('username', 'text')\\\n",
    "    .groupBy('username')\\\n",
    "    .agg(collect_list(\"text\").alias('raw_text'))\\\n",
    "    .withColumn('all_text', merge_text_udf('raw_text'))\\\n",
    "    .withColumn(\"processed_text\", process_text_udf('all_text'))\\\n",
    "    .withColumn('user_tokens', separate_udf('processed_text'))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"user_tokens\", \n",
    "                     outputCol=\"raw_features\", \n",
    "                     vocabSize=vocab_size, \n",
    "                     minTF=min_tf)\n",
    "cv_model = cv.fit(data4topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trasformdUserSlicesDf = cv_model.transform(data4topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = SparseVector(vocab_size, [])\n",
    "def filt(vector):\n",
    "    if vector == a:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "www = udf(filt, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trasformdUserSlicesDf = trasformdUserSlicesDf\\\n",
    "    .withColumn('tmp', www('raw_features'))\n",
    "trasformdUserSlicesDf = trasformdUserSlicesDf\\\n",
    "    .filter(trasformdUserSlicesDf.tmp == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying tf-idf\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying tf-idf\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=1)\n",
    "idf_model = idf\\\n",
    "    .fit(trasformdUserSlicesDf)\n",
    "posts_aggregated = idf_model\\\n",
    "    .transform(trasformdUserSlicesDf.drop('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_vector(col):\n",
    "    f = spark._jvm.com.example.spark.udfs.udfs.as_vector()\n",
    "    return Column(f.apply(_to_seq(spark, [col], _to_java_column)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform modelling for topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perform_topic_modelling(posts_aggregated,\n",
    "    num_topics, \n",
    "    max_iterations, \n",
    "    lda_model_path, \n",
    "    lda_topics_path, \n",
    "    lda_transformed_docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Username and his topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User_Topic RDD here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4topic.unpersist()\n",
    "user_topic = spark.read.parquet(lda_transformed_docs_path)\\\n",
    "    .withColumnRenamed('screename', 'username')\\\n",
    "    .join(spark.read.parquet(lda_topics_path), col('main_topic')==col('topic'))\\\n",
    "    .select(col('username'), col('topic'), col('terms'))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specification of features for further clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "feats = ['mean', 'var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [s+'_'+f for s in sentiments for f in feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tmp RDD for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=features, outputCol=\"features\")   \n",
    "data4cluster = vecAssembler.transform(data\\\n",
    "    .dropDuplicates(['username'])\\\n",
    "    .select(*[col(s)[f].alias(s+'_'+f) for s in sentiments for f in feats], 'username'))\\\n",
    "    .dropna(subset=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 clusters here\n",
    "kmeans = KMeans(k=15, seed=1)\\\n",
    "    .setFeaturesCol('features')\n",
    "model = kmeans\\\n",
    "    .fit(data4cluster\\\n",
    "         .dropna(subset=features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD with User->Cluster information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cluster = model\\\n",
    "    .transform(data4cluster\\\n",
    "         .dropna(subset=features))\\\n",
    "    .drop(*features, 'features')\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Joining all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- prediction: integer (nullable = false)\n",
      "\n",
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- topic: integer (nullable = true)\n",
      " |-- terms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_cluster.printSchema()\n",
    "user_topic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.dropDuplicates(['username'])\\\n",
    "        .join(user_cluster, 'username', 'outer')\\\n",
    "        .join(user_topic, 'username', 'outer')\\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.dropna(subset=['prediction', 'topic']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .parquet(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ////////\\\\\\\\\\\\\\\\\\\\\\\\\\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Visualisation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sizes2 = data.filter(col(\"len\").isNotNull()).filter(col(\"date_time\")>datetime.datetime(2017, 11, 7, 0, 0, 0)).select(col(\"len\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_sizes_old = data.filter(col(\"len\").isNotNull()).filter(col(\"date_time\")<datetime.datetime(2017, 11, 1, 0, 0, 0)).select(col(\"len\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Hist(_sizes_old, color='#00aced', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_Hist(_sizes2, color='#00aced', save=True, title='280_limit')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Spark]",
   "language": "python",
   "name": "conda-env-Spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
