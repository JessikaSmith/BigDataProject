{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import emot\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Bidirectional, Dropout, Activation, Dense\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import RMSprop, adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$#-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "p2 = re.compile('(RT @[a-zA-Z0-9_]+)')\n",
    "p3 = re.compile('(RT @[a-zA-Z0-9_]+:)')\n",
    "p4 = re.compile('(@[a-zA-Z0-9_]+)')\n",
    "p5 = re.compile('\\W+')\n",
    "p6 = re.compile('\\d+')\n",
    "reg = [p1, p2, p3, p4, p5, p6]\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "with open('data/EmoticonSentimentLexicon.txt') as file:\n",
    "    data = file.read()\n",
    "emoticons_sent = {x.split('\\t')[0]:int(x.split('\\t')[1]) for x in data.split('\\n')}\n",
    "emoticons_data = [x.split('\\t')[0] for x in data.split('\\n')]\n",
    "\n",
    "with open('data/EmojiSentimentLexicon.json') as file:\n",
    "    data = json.loads(file.read())\n",
    "emoji_sent = {x['emoji']:x['polarity'] for x in data}\n",
    "emoji_data = [x['emoji'] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear(text):\n",
    "    # all emoji stuff\n",
    "    emoji = [x['value'] for x in emot.emoji(text)]\n",
    "    emoticons = [x for x in emoticons_data if x in text]\n",
    "    for e in emoji:\n",
    "        if emoji_sent.get(e, 0) > 0:\n",
    "            text = text.replace(e, ' posemoji ')\n",
    "        elif emoji_sent.get(e, 0) < 0:\n",
    "            text = text.replace(e, ' negemoji ')\n",
    "        else:\n",
    "            text = text.replace(e, ' emoji ')\n",
    "    for e in emoticons:\n",
    "        if emoticons_sent.get(e, 0) > 0:\n",
    "            text = text.replace(e, ' posemoji ')\n",
    "        elif emoticons_sent.get(e, 0) < 0:\n",
    "            text = text.replace(e, ' negemoji ')\n",
    "        else:\n",
    "            text = text.replace(e, ' emoji ')\n",
    "    \n",
    "    # additional regex\n",
    "    text = re.sub(r\"#(\\w+)\", \" tag \", text)\n",
    "    text = re.sub(\"\\d+\", \" num \", text)\n",
    "    \n",
    "    for pattern in reg:\n",
    "        text = re.sub(pattern, ' ', text)\n",
    "    \n",
    "    text = [word for word in text.split(' ') if word != '']\n",
    "    \n",
    "    # MOAR\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
    "    pos_tokens = pos_tag(text)\n",
    "    \n",
    "    text = [word + '_' + tag for word, tag in pos_tokens]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', encoding='latin-1', names=['lable', 'id', 'date', 'wtf', 'name', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_length = 50\n",
    "vocabular = 20000\n",
    "embedding = 300\n",
    "hidden_size = 16\n",
    "batch_size = 64\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['x'] = df.text.apply(lambda a: clear(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['y'] = df.lable.apply(lambda b: 0 if b == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tmp = df.x.values.tolist()\n",
    "y_tmp = df.y.values.tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocabular)\n",
    "tokenizer.fit_on_texts(x_tmp)\n",
    "\n",
    "x = tokenizer.texts_to_sequences(x_tmp)\n",
    "x = pad_sequences(x, maxlen=sentence_length)\n",
    "\n",
    "y = to_categorical(y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabular + 1, embedding, \n",
    "                       input_length=sentence_length, trainable=True))\n",
    "    model.add(Bidirectional(LSTM(hidden_size, recurrent_dropout=0.5)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    opt = adam(lr=0.001, decay=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                 optimizer=opt,\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79999 samples, validate on 20001 samples\n",
      "Epoch 1/2\n",
      "79999/79999 [==============================] - 192s 2ms/step - loss: 0.5513 - acc: 0.7225 - val_loss: 0.4962 - val_acc: 0.7582\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75816, saving model to model/model.hdf5\n",
      "Epoch 2/2\n",
      "79999/79999 [==============================] - 192s 2ms/step - loss: 0.4582 - acc: 0.7949 - val_loss: 0.4976 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75816 to 0.75931, saving model to model/model.hdf5\n",
      "Train on 79999 samples, validate on 20001 samples\n",
      "Epoch 1/2\n",
      "79999/79999 [==============================] - 204s 3ms/step - loss: 0.5521 - acc: 0.7243 - val_loss: 0.4996 - val_acc: 0.7582\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75821, saving model to model/model.hdf5\n",
      "Epoch 2/2\n",
      "79999/79999 [==============================] - 203s 3ms/step - loss: 0.4600 - acc: 0.7941 - val_loss: 0.5000 - val_acc: 0.7605\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75821 to 0.76051, saving model to model/model.hdf5\n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/2\n",
      "80000/80000 [==============================] - 200s 2ms/step - loss: 0.5539 - acc: 0.7201 - val_loss: 0.4908 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76640, saving model to model/model.hdf5\n",
      "Epoch 2/2\n",
      "80000/80000 [==============================] - 202s 3ms/step - loss: 0.4602 - acc: 0.7934 - val_loss: 0.4929 - val_acc: 0.7646\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Train on 80001 samples, validate on 19999 samples\n",
      "Epoch 1/2\n",
      "80001/80001 [==============================] - 204s 3ms/step - loss: 0.5518 - acc: 0.7236 - val_loss: 0.4971 - val_acc: 0.7607\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76069, saving model to model/model.hdf5\n",
      "Epoch 2/2\n",
      "80001/80001 [==============================] - 199s 2ms/step - loss: 0.4562 - acc: 0.7953 - val_loss: 0.4952 - val_acc: 0.7621\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76069 to 0.76209, saving model to model/model.hdf5\n",
      "Train on 80001 samples, validate on 19999 samples\n",
      "Epoch 1/2\n",
      "80001/80001 [==============================] - 198s 2ms/step - loss: 0.5506 - acc: 0.7251 - val_loss: 0.4981 - val_acc: 0.7602\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76019, saving model to model/model.hdf5\n",
      "Epoch 2/2\n",
      "80001/80001 [==============================] - 195s 2ms/step - loss: 0.4566 - acc: 0.7969 - val_loss: 0.4981 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76019 to 0.76089, saving model to model/model.hdf5\n"
     ]
    }
   ],
   "source": [
    "global_precision = []\n",
    "global_f1 = []\n",
    "global_recall = []\n",
    "global_res = []\n",
    "confusion = []\n",
    "out_real = []\n",
    "out_pred = []\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "for train, test in kfold.split(np.zeros(x.shape), df.lable.tolist()):\n",
    "    checkpoint = ModelCheckpoint('model/model.hdf5', \n",
    "                             monitor='val_acc', save_best_only=True,\n",
    "                             verbose=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                   patience=15,  mode='auto',\n",
    "                                   verbose=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                  patience=5, min_lr=0.001,\n",
    "                                  verbose=True)\n",
    "    callbacks = [checkpoint]\n",
    "    model = create_model()\n",
    "    history = model.fit(\n",
    "        x[train], y[train],\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x[test], y[test]),\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    model = load_model(\"model/model.hdf5\")\n",
    "    \n",
    "    results = model.predict(x[test])\n",
    "    confusion.append(confusion_matrix(y[test].argmax(-1), results.argmax(-1)))\n",
    "    global_precision.append(precision_score(y[test].argmax(-1), results.argmax(-1), average=None))\n",
    "    global_f1.append(f1_score(y[test].argmax(-1), results.argmax(-1), average=None))\n",
    "    global_recall.append(recall_score(y[test].argmax(-1), results.argmax(-1), average=None))\n",
    "    global_res.append([global_precision, global_f1, global_recall])\n",
    "    \n",
    "    out_pred.extend(results)\n",
    "    out_real.extend(y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77196913, 0.75297264],\n",
       "       [0.75725915, 0.76613111],\n",
       "       [0.74351626, 0.78014279]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(global_res[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0.75845555, 0.76017238]),\n",
       "  array([0.77618593, 0.74654944]),\n",
       "  array([0.7913291 , 0.74544451]),\n",
       "  array([0.77101449, 0.75374794]),\n",
       "  array([0.7628606 , 0.75894893])],\n",
       " [array([0.75951644, 0.75910729]),\n",
       "  array([0.75332166, 0.76729499]),\n",
       "  array([0.75575073, 0.77615945]),\n",
       "  array([0.75791188, 0.76612269]),\n",
       "  array([0.75979506, 0.76197113])],\n",
       " [array([0.76058029, 0.75804517]),\n",
       "  array([0.73176588, 0.78922646]),\n",
       "  array([0.72323394, 0.80951429]),\n",
       "  array([0.74524715, 0.77891054]),\n",
       "  array([0.75675405, 0.76501749])]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
